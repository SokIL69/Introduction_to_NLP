{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку естественного языка\n",
    "# Урок 2. Создание признакового пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ 2\n",
    "\n",
    "### Задание 1.\n",
    "Задание: обучите три классификатора:\n",
    "\n",
    "1) на токенах с высокой частотой\n",
    "\n",
    "2) на токенах со средней частотой\n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации.\n",
    "\n",
    "### Задание 2.\n",
    "найти фичи с наибольшей значимостью, и вывести их\n",
    "\n",
    "### Задание 3.\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера\n",
    "\n",
    "3) убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнил ___Соковнин ИЛ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы по дз\n",
    "\n",
    "### Задание 1:\n",
    "Самый лучший результат получался по полному набору токенов.\n",
    "Хороший результат получился при использовании наиболее популярных токенов, как для CountVectorizer, так и для TfidfVectorizer. \n",
    "\n",
    "### Задание 2:\n",
    "Фичи с наибольшей значимостью несколько отличаются для разных частот. \n",
    "\n",
    "### Задание 3:\n",
    "1. Лучше всего отработал TfIdfVectorizer.\n",
    "2. HashingVectorizer приближается к лучшему результату на больших размерах > 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сброс ограничений на количество символов в записи\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 \n",
      " 2 \n",
      " Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so ... \n",
      "\n",
      "__label__2 \n",
      " 2 \n",
      " The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'ga ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/corpus', 'r') as f:\n",
    "    i = 0\n",
    "    for l in f.readlines():\n",
    "        if i < 2:\n",
    "            t = l.split(' ', 1)\n",
    "            t0 = t[0][-1]\n",
    "            t1 = t[1][:100]\n",
    "            print(t[0], '\\n', t0, '\\n', t1, '... \\n')\n",
    "\n",
    "            i += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  \\\n",
       "0        2   \n",
       "1        2   \n",
       "2        2   \n",
       "3        2   \n",
       "4        2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n  \n",
       "1                                                                                                                                                                                                                                                             The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n  \n",
       "2  Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n  \n",
       "3                   Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n  \n",
       "4                                                                                                                                                                                                                                                                                         Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём dataframe из файла\n",
    "\n",
    "with open('./data/corpus', 'r') as f:\n",
    "    df = pd.DataFrame({'category': t[0][-1], 'text': t[1]} for t in (l.split(' ', 1) for l in f.readlines()))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize as tknz\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "punctuation_marks = list(punctuation)\n",
    "noise = set(stop_words + punctuation_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(token, noise):\n",
    "    \"\"\" \n",
    "    Удаляем шум из токенов.\n",
    "    \"\"\"\n",
    "    \n",
    "    remove_sw = [word for word in token if not word in noise]\n",
    "\n",
    "    return  remove_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лематизация\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_lemmatizer(words, lemmatizer, pos):\n",
    "    \"\"\"\n",
    "    lemmitization\n",
    "    \"\"\"\n",
    "\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemmas.append(lemmatizer.lemmatize(word, pos = nltk.corpus.wordnet.VERB) )\n",
    "         \n",
    "    return lemmas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n</td>\n",
       "      <td>[Stuning, even, non-gamer, This, sound, track, beautiful, It, paint, senery, mind, well, I, would, recomend, even, people, hate, vid, game, music, I, play, game, Chrono, Cross, game, I, ever, play, best, music, It, back, away, crude, keyboarding, take, fresher, step, grate, guitars, soulful, orchestras, It, would, impress, anyone, care, listen, ^_^]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n</td>\n",
       "      <td>[The, best, soundtrack, ever, anything, I, 'm, read, lot, review, say, best, 'game, soundtrack, I, figure, I, 'd, write, review, disagree, bite, This, opinino, Yasunori, Mitsuda, 's, ultimate, masterpiece, The, music, timeless, I, 'm, listen, years, beauty, simply, refuse, fade.The, price, tag, pretty, stagger, I, must, say, go, buy, cd, much, money, one, I, feel, would, worth, every, penny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n</td>\n",
       "      <td>[Amazing, This, soundtrack, favorite, music, time, hand, The, intense, sadness, ``, Prisoners, Fate, '', mean, 've, play, game, hope, ``, A, Distant, Promise, '', ``, Girl, Stole, Star, '', important, inspiration, personally, throughout, teen, years, The, higher, energy, track, like, ``, Chrono, Cross, Time, 's, Scar~, '', ``, Time, Dreamwatch, '', ``, Chronomantique, '', indefinably, remeniscent, Chrono, Trigger, absolutely, superb, well.This, soundtrack, amaze, music, probably, best, composer, 's, work, I, n't, hear, Xenogears, soundtrack, I, ca, n't, say, sure, even, 've, never, play, game, would, worth, twice, price, buy, it.I, wish, I, could, give, 6, star]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  \\\n",
       "0        2   \n",
       "1        2   \n",
       "2        2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n   \n",
       "1                                                                                                                                                                                                                                                             The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n   \n",
       "2  Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text_tokens  \n",
       "0                                                                                                                                                                                                                                                                                                                                 [Stuning, even, non-gamer, This, sound, track, beautiful, It, paint, senery, mind, well, I, would, recomend, even, people, hate, vid, game, music, I, play, game, Chrono, Cross, game, I, ever, play, best, music, It, back, away, crude, keyboarding, take, fresher, step, grate, guitars, soulful, orchestras, It, would, impress, anyone, care, listen, ^_^]  \n",
       "1                                                                                                                                                                                                                                                                                      [The, best, soundtrack, ever, anything, I, 'm, read, lot, review, say, best, 'game, soundtrack, I, figure, I, 'd, write, review, disagree, bite, This, opinino, Yasunori, Mitsuda, 's, ultimate, masterpiece, The, music, timeless, I, 'm, listen, years, beauty, simply, refuse, fade.The, price, tag, pretty, stagger, I, must, say, go, buy, cd, much, money, one, I, feel, would, worth, every, penny]  \n",
       "2  [Amazing, This, soundtrack, favorite, music, time, hand, The, intense, sadness, ``, Prisoners, Fate, '', mean, 've, play, game, hope, ``, A, Distant, Promise, '', ``, Girl, Stole, Star, '', important, inspiration, personally, throughout, teen, years, The, higher, energy, track, like, ``, Chrono, Cross, Time, 's, Scar~, '', ``, Time, Dreamwatch, '', ``, Chronomantique, '', indefinably, remeniscent, Chrono, Trigger, absolutely, superb, well.This, soundtrack, amaze, music, probably, best, composer, 's, work, I, n't, hear, Xenogears, soundtrack, I, ca, n't, say, sure, even, 've, never, play, game, would, worth, twice, price, buy, it.I, wish, I, could, give, 6, star]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# токенизация\n",
    "# удалим стоп слова и знаки препинания\n",
    "# лематизация\n",
    "df['text_tokens'] = df['text'].apply(tknz.word_tokenize)\n",
    "df['text_tokens'] = df['text_tokens'].apply(remove_noise, noise=noise)\n",
    "df['text_tokens'] = \\\n",
    "        df['text_tokens'].apply(get_lemmatizer, lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stuning', 'even', 'non-gamer', 'This', 'sound']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим словарь наших текстов\n",
    "dictionary = []\n",
    "for ts in df.text_tokens:\n",
    "    for t in ts:\n",
    "        dictionary.append(t)\n",
    "        \n",
    "dictionary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Одной строкой\n",
    "#\n",
    "dictionary = [ t for ts in df.text_tokens for t in ts ]\n",
    "# dictionary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stuning': 1, 'even': 1249, 'non-gamer': 1, 'This': 3556, 'sound': 645}\n"
     ]
    }
   ],
   "source": [
    "# Подсчитать частоту слов в списке и отсортировать по частоте\n",
    "counts = Counter(dictionary)\n",
    "# counts.items()\n",
    "print(dict(list(counts.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 21131), ('book', 7347), (\"'s\", 5758), ('The', 5349), (\"n't\", 5297)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сортировка по частоте\n",
    "sorted_counts = sorted(counts.items(), key=lambda item: (-item[1]))\n",
    "sorted_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'book', \"'s\", 'The', \"n't\", 'This', \"''\", 'read', 'It', 'one']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Частотный словарь\n",
    "freq_dictionary = list(tp[0] for tp in sorted_counts)\n",
    "freq_dictionary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# То же самое одной строкой\n",
    "#\n",
    "freq_dicts = list(tp[0] for tp in sorted(Counter(dictionary).items(), key=lambda x: -x[1]))\n",
    "# freq_dictionary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим четыре набора\n",
    "freq_dicts = { \n",
    "    'all': set(freq_dicts),\n",
    "    'high frequency': set(freq_dicts[:len(freq_dicts)//20]),  # < 5%\n",
    "    'medium frequency': set(freq_dicts[len(freq_dicts)//20 : len(freq_dicts)//5]),  # от 5 до 20%\n",
    "    'low frequency': set(freq_dicts[len(freq_dicts)//5:]),  # > 20%\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_dicts['high frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаём и обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Извлечение фичей из текстовых данных - векторизаторы\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Создаём тренировочный и тестовый наборы данных\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_filtered_text(df_, freq_type):\n",
    "    '''\n",
    "    Фильтрация текстов по частотному словарю\n",
    "    '''\n",
    "    \n",
    "    filter_text_tokens = []\n",
    "    for tt in df_['text_tokens']:\n",
    "        filter_tokens = []\n",
    "        for t in tt:\n",
    "            if t in freq_dicts[freq_type]:\n",
    "                filter_tokens.append(t)\n",
    "        filter_text_tokens.append(' '.join(filter_tokens))\n",
    "    \n",
    "    return filter_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filtered_text(df_, freq_type):\n",
    "#     return [ ' '.join(t for t in tt if t in freq_dicts[freq_type]) for tt in df_['text_tokens'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Though one reviewer felt format book poor choice I find perfect I leave copy one bag I pick new two store I find let find I like book little think much cook advance combine heat time come This do many recipes call `` cook rice '' `` cook '' I find also allow use quickly new note though book mean Many recipes call products lovely lovely book must try live earth\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_type = 'high frequency'\n",
    "freq_dicts['high frequency']\n",
    "frequency_filtered_text(df, freq_type)[0]\n",
    "frequency_filtered_text(df_train, freq_type)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_fit_and_test(freq_type):\n",
    "\n",
    "    # Тренировочный набор данных\n",
    "    x_train = frequency_filtered_text(df_train, freq_type)\n",
    "    y_train = df_train['category']\n",
    "\n",
    "    # Тестовый набор данных\n",
    "    x_test = frequency_filtered_text(df_test, freq_type)\n",
    "    y_test = df_test['category']\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификаторы\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_important_features(vec, model):\n",
    "    '''\n",
    "    Находим и выводим фичи с наибольшей значимостью    \n",
    "\n",
    "    '''\n",
    "\n",
    "    # hasattr(obj, name) - возвращает флаг, указывающий на то, содержит ли объект указанный атрибут.\n",
    "    if vec is not None and hasattr(vec, 'get_feature_names'):\n",
    "        feature_names = vec.get_feature_names()\n",
    "        # print(feature_names), print(model.coef_[0])\n",
    "\n",
    "        # Создаём отсортированный  zip-объект ( список кортежей - [(), (), ...] )\n",
    "        # [(-2.3982348391433557, 'poor'), (-1.9768447288749496, 'worst'), (-1.9529763977726964, 'bore'), ...]\n",
    "        coefs_with_importances = sorted(zip(model.coef_[0], feature_names))  #  zip-объект (список кортежей) [(), (), ...]\n",
    "        \n",
    "        # Выводим n_important фичей\n",
    "        n_important = 10;\n",
    "\n",
    "        print(\"\\nФичи с наибольшей положительной значимостью: \")        \n",
    "        for feature in reversed(coefs_with_importances[-n_important:]):\n",
    "            print(f\"{feature[1]} : {feature[0]:.3f}\")\n",
    "\n",
    "        print(\"\\nФичи с наибольшей отрицательной значимостью: \")\n",
    "        for feature in coefs_with_importances[:n_important]:\n",
    "            print(f\"{feature[1]} : {feature[0]:.3f}\")\n",
    "       \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test(freq_type, vec=None, model=None):\n",
    "    '''\n",
    "    Обучаем и тестируем модель\n",
    "    \n",
    "    vec - векторайзер \n",
    "    model - классификатор    \n",
    "    '''\n",
    "    \n",
    "    print('Частоты слов: ' + freq_type)\n",
    "    x_train, y_train, x_test, y_test = get_fit_and_test(freq_type)\n",
    "\n",
    "    # Задаём классификатор по умолчанию\n",
    "    if model == None:\n",
    "        model = LogisticRegression(random_state=42)  # Логистическая регрессия\n",
    "\n",
    "    # bow - bag of words (мешок слов)\n",
    "    bow = vec.fit_transform(x_train)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(bow, y_train)\n",
    "    \n",
    "    # Выводим наиболее важные фичи\n",
    "    print_important_features(vec, model)\n",
    "    \n",
    "    # Генерируем прогнозы\n",
    "    pred = model.predict(vec.transform(x_test))\n",
    "    \n",
    "    # Строим текстовый отчет по основным показателям классификации.\n",
    "    # В отчете отображается точность, частота отзыва, значение F1 и другая информация по каждой категории.\n",
    "    print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "high frequency\n",
      "medium frequency\n",
      "low frequency\n"
     ]
    }
   ],
   "source": [
    "for freq_type in freq_dicts:\n",
    "    print(freq_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Задание: обучите три классификатора:\n",
    "\n",
    "1. на токенах с высокой частотой\n",
    "2. на токенах со средней частотой\n",
    "3. на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации.# CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "### Задание 2.\n",
    "найти фичи с наибольшей значимостью, и вывести их\n",
    "\n",
    "### Задание 3.\n",
    "1. сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "2. подобрать оптимальный размер для hashing векторайзера\n",
    "3. убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer:\n",
      "Частоты слов: all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "excellent : 2.366\n",
      "perfect : 2.014\n",
      "government : 1.428\n",
      "awesome : 1.416\n",
      "amaze : 1.332\n",
      "wonderful : 1.328\n",
      "today : 1.273\n",
      "love : 1.270\n",
      "great : 1.266\n",
      "works : 1.264\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "poor : -2.398\n",
      "worst : -1.977\n",
      "bore : -1.953\n",
      "waste : -1.934\n",
      "boring : -1.871\n",
      "disappoint : -1.641\n",
      "not : -1.546\n",
      "disappointment : -1.518\n",
      "awful : -1.456\n",
      "useless : -1.443\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.85      0.86      1054\n",
      "           2       0.83      0.85      0.84       946\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "Частоты слов: high frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "excellent : 2.262\n",
      "perfect : 2.046\n",
      "intense : 1.787\n",
      "fantastic : 1.650\n",
      "government : 1.580\n",
      "brown : 1.572\n",
      "finest : 1.561\n",
      "works : 1.496\n",
      "debut : 1.484\n",
      "heart : 1.475\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "boring : -2.804\n",
      "worst : -2.477\n",
      "poor : -2.413\n",
      "disappointment : -2.364\n",
      "bore : -2.112\n",
      "mislead : -2.035\n",
      "beware : -1.968\n",
      "errors : -1.848\n",
      "too : -1.842\n",
      "waste : -1.814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.82      0.83      1049\n",
      "           2       0.81      0.82      0.81       951\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.82      0.82      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n",
      "Частоты слов: medium frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "awsome : 1.708\n",
      "epic : 1.600\n",
      "solid : 1.482\n",
      "medical : 1.454\n",
      "dog : 1.427\n",
      "clan : 1.426\n",
      "attend : 1.405\n",
      "outstanding : 1.392\n",
      "investment : 1.389\n",
      "mature : 1.387\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "awful : -1.957\n",
      "wrong : -1.894\n",
      "junk : -1.830\n",
      "horribly : -1.743\n",
      "stain : -1.622\n",
      "sadly : -1.602\n",
      "false : -1.600\n",
      "essentially : -1.583\n",
      "drivel : -1.574\n",
      "contrive : -1.566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.69      0.67       972\n",
      "           2       0.69      0.64      0.66      1028\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.67      0.67      0.66      2000\n",
      "weighted avg       0.67      0.66      0.66      2000\n",
      "\n",
      "Частоты слов: low frequency\n",
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "heart : 1.703\n",
      "excelent : 1.161\n",
      "must : 1.157\n",
      "highly : 1.119\n",
      "future : 1.092\n",
      "own : 1.065\n",
      "rock : 1.026\n",
      "eerily : 1.014\n",
      "wife : 0.982\n",
      "war : 0.977\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "money : -1.239\n",
      "pseudo : -1.151\n",
      "harlequin : -1.137\n",
      "crappy : -1.078\n",
      "bootleg : -1.040\n",
      "started : -1.021\n",
      "slow : -1.001\n",
      "half : -1.000\n",
      "disapointing : -0.989\n",
      "boo : -0.989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.59      0.65      1271\n",
      "           2       0.46      0.61      0.53       729\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.60      0.60      0.59      2000\n",
      "weighted avg       0.63      0.60      0.61      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('CountVectorizer:')\n",
    "for freq_type in freq_dicts:\n",
    "    fit_and_test(freq_type, CountVectorizer(ngram_range=(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer:\n",
      "Частоты слов: all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "great : 7.558\n",
      "love : 5.946\n",
      "excellent : 5.098\n",
      "best : 4.451\n",
      "good : 3.941\n",
      "perfect : 3.495\n",
      "well : 3.472\n",
      "easy : 3.065\n",
      "wonderful : 2.944\n",
      "must : 2.743\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "not : -5.304\n",
      "waste : -5.019\n",
      "bore : -4.500\n",
      "poor : -4.355\n",
      "worst : -4.309\n",
      "disappoint : -4.233\n",
      "bad : -4.231\n",
      "money : -3.696\n",
      "return : -2.944\n",
      "nothing : -2.828\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.85      0.86      1071\n",
      "           2       0.83      0.86      0.85       929\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n",
      "Частоты слов: high frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "great : 6.629\n",
      "love : 5.375\n",
      "excellent : 5.158\n",
      "best : 4.191\n",
      "perfect : 3.598\n",
      "good : 3.567\n",
      "well : 2.979\n",
      "wonderful : 2.972\n",
      "easy : 2.949\n",
      "amaze : 2.722\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "not : -5.035\n",
      "waste : -4.888\n",
      "bore : -4.641\n",
      "poor : -4.523\n",
      "worst : -4.447\n",
      "disappoint : -4.232\n",
      "bad : -3.823\n",
      "money : -3.410\n",
      "terrible : -2.913\n",
      "boring : -2.832\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.85      0.86      1063\n",
      "           2       0.83      0.85      0.84       937\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "Частоты слов: medium frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "epic : 1.814\n",
      "medical : 1.813\n",
      "awsome : 1.698\n",
      "attend : 1.637\n",
      "fast : 1.510\n",
      "ya : 1.486\n",
      "ease : 1.473\n",
      "turner : 1.465\n",
      "outstanding : 1.422\n",
      "amazing : 1.400\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "awful : -2.156\n",
      "wrong : -2.058\n",
      "junk : -1.921\n",
      "false : -1.897\n",
      "sadly : -1.741\n",
      "none : -1.673\n",
      "drivel : -1.642\n",
      "missing : -1.631\n",
      "horribly : -1.563\n",
      "dissapointing : -1.526\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.69      0.71      1100\n",
      "           2       0.65      0.69      0.67       900\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.69      0.69      0.69      2000\n",
      "weighted avg       0.70      0.69      0.69      2000\n",
      "\n",
      "Частоты слов: low frequency\n",
      "\n",
      "Фичи с наибольшей положительной значимостью: \n",
      "heart : 1.609\n",
      "well : 1.252\n",
      "must : 1.195\n",
      "highly : 1.123\n",
      "rock : 1.093\n",
      "excelent : 1.053\n",
      "great : 0.995\n",
      "provoking : 0.962\n",
      "own : 0.960\n",
      "handy : 0.942\n",
      "\n",
      "Фичи с наибольшей отрицательной значимостью: \n",
      "money : -1.368\n",
      "re : -1.361\n",
      "what : -1.122\n",
      "pseudo : -1.090\n",
      "00 : -1.065\n",
      "harlequin : -1.059\n",
      "there : -1.048\n",
      "either : -1.043\n",
      "self : -1.041\n",
      "half : -1.039\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.60      0.66      1289\n",
      "           2       0.46      0.62      0.53       711\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.60      0.61      0.59      2000\n",
      "weighted avg       0.64      0.60      0.61      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('TfidfVectorizer:')\n",
    "for freq_type in freq_dicts:\n",
    "    fit_and_test(freq_type, TfidfVectorizer(ngram_range=(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingVectorizer with 100 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.66      0.66      1024\n",
      "           2       0.64      0.63      0.64       976\n",
      "\n",
      "    accuracy                           0.65      2000\n",
      "   macro avg       0.65      0.65      0.65      2000\n",
      "weighted avg       0.65      0.65      0.65      2000\n",
      "\n",
      "HashingVectorizer with 200 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.70      0.70      1022\n",
      "           2       0.68      0.67      0.68       978\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.69      0.69      0.69      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n",
      "HashingVectorizer with 500 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.78      0.78      1032\n",
      "           2       0.77      0.76      0.76       968\n",
      "\n",
      "    accuracy                           0.77      2000\n",
      "   macro avg       0.77      0.77      0.77      2000\n",
      "weighted avg       0.77      0.77      0.77      2000\n",
      "\n",
      "HashingVectorizer with 1000 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.78      0.79      1078\n",
      "           2       0.75      0.79      0.77       922\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.78      0.78      0.78      2000\n",
      "weighted avg       0.78      0.78      0.78      2000\n",
      "\n",
      "HashingVectorizer with 10000 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.85      0.85      1050\n",
      "           2       0.83      0.84      0.84       950\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "HashingVectorizer with 100000 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.85      0.85      1051\n",
      "           2       0.83      0.84      0.84       949\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.85      0.84      0.85      2000\n",
      "\n",
      "HashingVectorizer with 1000000 features:\n",
      "Частоты слов: all\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.85      0.85      1052\n",
      "           2       0.83      0.84      0.84       948\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_features in [100, 200, 500, 1000, 10000, 100000, 1000000]:\n",
    "    print(f'HashingVectorizer with {n_features} features:')\n",
    "    fit_and_test('all', HashingVectorizer(analyzer='word', n_features=n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полносвязнная сетка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'I', 'book', \"'s\", 'The', \"n't\", 'This', \"''\", 'read']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5,   1,  26,   1, 921,   0,   0,   0,   0,   0],\n",
       "       [  2,   9,   5,   3,   0,   0,   0,   0,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer_0 = TextVectorization(  # Текстовый векторизационный слой\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=n_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=10\n",
    "    )\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = frequency_filtered_text(df, freq_type)\n",
    "vectorize_layer_0.adapt(text_data)  # создает \"словарь\".\n",
    "\n",
    "print(vectorize_layer_0.get_vocabulary()[:10])\n",
    "print()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "model.add(vectorize_layer_0)\n",
    "\n",
    "# print(vectorize_layer.get_vocabulary()[1], vectorize_layer.get_vocabulary()[921])\n",
    "input_data = [\"The foo go to bar\", \"I read The book\"]\n",
    "model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'I', 'book', \"'s\", 'The', \"n't\", 'This', \"''\", 'read']\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.layers.experimental.preprocessing. TextVectorization\n",
    "# https://spec-zone.ru/tensorflow~2.4/keras/layers/experimental/preprocessing/textvectorization\n",
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "max_len = 200  # Sequence length to pad the outputs to.\n",
    "embedding_dim = 200\n",
    "\n",
    "vectorize_layer = TextVectorization(  # Текстовый векторизационный слой\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=n_features,  # Maximum vocab size\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = frequency_filtered_text(df, freq_type)\n",
    "vectorize_layer.adapt(text_data)  # создает \"словарь\".\n",
    "print(vectorize_layer.get_vocabulary()[:10])\n",
    "\n",
    "\n",
    "# ЗАДАЧА КЛАССИФИКАЦИИ ТЕКСТОВЫХ ДАННЫХ С WORD EMBEDDINGS В TENSORFLOW\n",
    "# https://python-school.ru/blog/nlp-classification-with-emdeddings/\n",
    "def build_nn_vectorizer(n_features, freq_type='all'):\n",
    "    \n",
    "    # Create the model that uses the vectorize text layer\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Start by creating an explicit input layer. It needs to have a shape of\n",
    "    # (1,) (because we need to guarantee that there is exactly one string\n",
    "    # input per batch), and the dtype needs to be 'string'.\n",
    "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "    \n",
    "    # The first layer in our model is the vectorization layer. After this\n",
    "    # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "    # indices.\n",
    "    model.add(vectorize_layer) \n",
    "    \n",
    "    model.add(Embedding(\n",
    "                input_dim=n_features,  #  размер словаря = n_features\n",
    "                output_dim=embedding_dim,  # размерность выходной матрицы Embedding.\n",
    "                input_length=max_len  #  размерность входного слоя. \n",
    "    ))\n",
    "    \n",
    "    model.add(Flatten())  # слой Flatten, который выпрямляет слой Embdedding;\n",
    "    model.add(Dense(1, activation='sigmoid'))  # один выходной нейрон с функцией активацией sigmoid,\n",
    "                                               # который выводит вероятность принадлежности к классу 1 (позитивный отзыв)\n",
    "                                               # или 0 (негативный отзыв).\n",
    "\n",
    "#     model.compile(optimizer='adam',  # оптимизатор\n",
    "#                   loss=tf.keras.losses.BinaryCrossentropy()  # функцию потерь\n",
    "#                  ) \n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,) (2000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Though one reviewer felt format book poor choice I find perfect I leave copy one bag I pick new two store I find let find I like book little think much cook advance combine heat time come This do many recipes call `` cook rice '' `` cook '' I find also allow use quickly new note though book mean Many recipes call products lovely lovely book must try live earth\",\n",
       " '2')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = frequency_filtered_text(df_train, freq_type)\n",
    "y_train = df_train['category'].tolist()\n",
    "\n",
    "x_test = frequency_filtered_text(df_test, freq_type)\n",
    "y_test = df_test['category'].tolist()\n",
    "\n",
    "\n",
    "print(np.array(x_train).shape, np.array(y_test).shape)\n",
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_62 (Text  (None, 200)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    (None, 200, 200)          2000000   \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 40000)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 40001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,040,001\n",
      "Trainable params: 2,040,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_nn_vectorizer(10000)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-182-9c3ab290dbab>\", line 6, in <module>\n      history = model.fit(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1385, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1022, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1011, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1001, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 861, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 919, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1922, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_68669]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-9c3ab290dbab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     metrics=['accuracy'])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-182-9c3ab290dbab>\", line 6, in <module>\n      history = model.fit(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1385, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1022, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1011, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1001, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 861, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 919, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1922, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_68669]"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "    \n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=15,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "### Задание 1:\n",
    "Самый лучший результат получался по полному набору токенов.\n",
    "Хороший результат получился при использовании наиболее популярных токенов, как для CountVectorizer, так и для TfidfVectorizer. \n",
    "\n",
    "### Задание 2:\n",
    "Фичи с наибольшей значимостью несколько отличаются для разных частот. \n",
    "\n",
    "### Задание 3:\n",
    "1. Лучше всего отработал TfIdfVectorizer.\n",
    "2. HashingVectorizer приближается к лучшему результату на больших размерах > 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (causalml)",
   "language": "python",
   "name": "causalml-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
